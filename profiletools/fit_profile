#!/usr/bin/env python2.7
# Copyright 2014 Mark Chilenski
# This program is distributed under the terms of the GNU General Purpose License (GPL).
# Refer to http://www.gnu.org/licenses/gpl.txt
# 
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# 
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import division
import argparse
import sys
import os.path
import csv
import scipy
import scipy.io
import profiletools
import eqtools
import matplotlib.pyplot as plt
import matplotlib.widgets as mplw
import matplotlib.gridspec as mplgs
import multiprocessing
import inspect
import socket
import getpass
import time

__version__ = '0.0'

class CustomFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawDescriptionHelpFormatter):
    pass

parser = argparse.ArgumentParser(
    description=
"""Fit univariate profile using gptools/profiletools.

Calling without arguments will enter an interactive mode, but it will only
prompt you for the most basic parameters needed. Use the command line flags to
specify more advanced options. This program can operate on ne, Te data from the
tree, or you can provide data in a NetCDF or CSV file.

You can choose whether to average over a time window, a specific set of points
or to use a single time slice. Even if providing data in a file, you should
specify the shot number and time window so that appropriate constraints can be
imposed on the fit at the magnetic axis and edge.

Examples:

Basic way to fit ne profile from shot 1101014006, averaged over the flat top
from 0.965s to 1.365s, using core and edge TS:

    fit_profile --shot 1101014006 --signal ne --t-min 0.965 --t-max 1.365

(To specify specific diagnostics to include, use the --system keyword.)

Basic way to fit data from NetCDF file foo.nc, assuming the data are from the
time window 0.965s to 1.365s of shot 1101014006. The abscissa is specified to be
normalized poloidal flux and is stored in the variable psin in the NetCDF file.
The ordinate is stored in the variable q and its uncertainty in err_q:

    fit_profile -i foo.nc --coordinate psinorm --t-min 0.965 --t-max 1.365 -x psin -y q

When the fit is complete, you will be presented with a plot of the fit and its
uncertainty. You will then have the option to reject the fit and exit (with
status code 2), or to accept the fit and exit (with status code 0). Upon
accepting the fit, the data will be saved to either a NetCDF file or a CSV file,
depending on the extension of the output filename you specify.

Several things can go wrong in the fit. If there are bad points/outliers in your
data you can attempt to remove them by specifying one or both of
--change-threshold and --outlier-threshold. Change threshold rejects points that
are too distant from their neighbors, outlier threshold rejects points that are
too distant from the fit.

If there are not apparent outliers, but the fit still looks bad, then there is
likely an issue with the estimation of the fit's properties -- namely the
so-called hyperparameters that dictate the spatial correlation between points.
Try increasing the --random-starts flag to at least 8 as a first cut. This will
make the fit take quite a bit longer, but is parallelized, so the more cores
your computer has, the faster you will have your answer. If this still yields
unsatisfactory fits, try adjusting the bounds for the hyperparameters using
--bounds.

Note that many warnings regarding overflow in cosh and casting complex values
will be emitted -- these are usually benign. You will also see warnings that
the minimizer failed. These indicate that a particular random guess for the 
hyperparameters walked the minimizer into a bad state. At the end of the
optimization you will be told how many starts were accepted. Try to increase
--random-starts and/or adjust --bounds until this number is at least 4.

Note that, at this point, the program only supports handling of the
hyperparameters through MAP estimation. This is usually sufficient for the value
in the core, but is NOT sufficient for getting uncertainties on gradients (even
in the core) or detailed information on the edge.""",
    formatter_class=CustomFormatter
)
parser.add_argument('--signal',
                    choices=['ne', 'Te'],
                    help="Which signal to fit when pulling data from the tree.")
parser.add_argument('--shot',
                    type=int,
                    help="Shot number to use. Required when pulling data from "
                         "the tree. When pulling data from a file, this is "
                         "needed to specify constraints at the magnetic axis and "
                         "limiter.")
parser.add_argument('--t-min',
                    type=float,
                    help="Starting time of period to average over. If you are "
                         "reading data from a file, you can set this flag to "
                         "tell the program what time window to average over "
                         "when finding the location of the limiter/magnetic axis "
                         "when applying constraints.")
parser.add_argument('--t-max',
                    type=float,
                    help="Ending time of period to average over. If you are "
                         "reading data from a file, you can set this flag to "
                         "tell the program what time window to average over when "
                         "finding the location of the limiter/magnetic axis "
                         "when applying constraints.")
parser.add_argument('-t', '--t-points',
                    type=float,
                    nargs='+',
                    help="Individual time values to use. The nearest time to "
                         "each will be selected for each channel. You can use "
                         "this, for instance, to specify the times you have "
                         "determined are at a particular sawtooth/ELM phase. You "
                         "must either specify --t-min and --t-max, or -t.")
parser.add_argument('--npts',
                    type=int,
                    default=400,
                    help="Number of evenly-spaced points to evaluate the fit at.")
parser.add_argument('--x-min',
                    type=float,
                    default=0,
                    help="Starting point for the evenly-spaced points to evaluate "
                         "the fit at.")
parser.add_argument('--x-max',
                    type=float,
                    default=1.2,
                    help="Ending point for the evenly-spaced points to evaluate "
                         "the fit at.")
parser.add_argument('--x-pts',
                    type=float,
                    nargs='+',
                    help="Discrete points to evaluate the fit at. If present, "
                         "this overrides the effect of npts, x-min and x-max.")
parser.add_argument('--system',
                    nargs='+',
                    choices=['CTS', 'ETS', 'TS', 'GPC', 'GPC2', 'FRCECE'],
                    help="Which system(s) to take data from. If not provided, "
                         "all applicable systems will be used. The 'TS' option "
                         "is a shortcut to include both the core (CTS) and edge "
                         "(ETS) Thomson systems.")
parser.add_argument('--kernel',
                    choices=['gibbstanh', 'SE'],
                    default='gibbstanh',
                    help="Which covariance kernel to use. This dictates the "
                         "properties of the fit. SE is the squared exponential "
                         "kernel, which is good for core data. gibbstanh is the "
                         "Gibbs kernel with tanh warping of the length scale. "
                         "This kernel allows the entire profile to be fit at "
                         "once, and should be used if you have edge data. You "
                         "will typically also want to use --no-edge-constraint "
                         "if you specify the SE kernel. See also --core-only.")
parser.add_argument('--coordinate',
                    choices=['psinorm', 'Rmid', 'r/a', 'volnorm', 'phinorm',
                             'sqrtpsinorm', 'sqrtr/a', 'sqrtvolnorm', 'sqrtphinorm'],
                    default='',
                    help="Which coordinate to fit against. Defaults to psinorm "
                         "when pulling data from the tree. Used to determine how "
                         "to apply core/edge constraints when pulling data from "
                         "a file.")
parser.add_argument('--no-core-constraint',
                    action='store_true',
                    help="Set this flag to disable the slope=0 constraint at the "
                         "magnetic axis.")
parser.add_argument('--no-edge-constraint',
                    action='store_true',
                    help="Set this flag to disable the slope, value=0 constraint "
                         "at/outside the GH limiter.")
parser.add_argument('--core-only',
                    action='store_true',
                    help="Set this flag to only fit the data inside the LCFS. "
                         "This will switch to using a squared exponential kernel, "
                         "and will disable the edge value, slope constraints.")
parser.add_argument('--robust',
                    action='store_true',
                    help="Set this flag to use robust estimators (median, IQR) "
                         "when performing time-averages.")
parser.add_argument('--all-points', '--no-average',
                    action='store_true',
                    help="Set this flag to keep all points from the time window "
                         "selected instead of performing a time average. This "
                         "will make the fit take longer and is statistically-"
                         "questionable, but may be useful in some cases.")
parser.add_argument('--change-threshold',
                    type=float,
                    help="If provided, any points whose differences with respect "
                         "to either of their neighbors are more than this many "
                         "times their own error bar will be rejected. This is "
                         "useful for getting rid of bad channels. A value of 9 "
                         "is often useful.")
parser.add_argument('--outlier-threshold',
                    type=float,
                    help="If provided, any points whose values are more than this "
                         "many times their own error bar outside of the fit will "
                         "be rejected. A value of 3 is often useful.")
parser.add_argument('--random-starts',
                    type=int,
                    help="The number of random starts to use when trying to find "
                         "the MAP estimate for the hyperparameters. If you are "
                         "getting bad fits, try increasing this. If not "
                         "specified, this is set to the number of processors "
                         "available on your machine.")
parser.add_argument('--upper-factor',
                    type=float,
                    default=5,
                    help="Factor by which the range of the data is multiplied "
                         "to generate the upper bound on the hyperparameters. If "
                         "you are getting bad fits, try adjusting this.")
parser.add_argument('--lower-factor',
                    type=float,
                    default=5,
                    help="Factor by which the range of the data is divided to "
                         "generate the lower bound on the hyperparameters. If "
                         "you are getting bad fits, try adjusting this.")
parser.add_argument('--bounds',
                    type=float,
                    nargs='+',
                    help="Bounds to use for each of the hyperparameters. "
                         "Specified as pairs of lower, upper bounds. If present, "
                         "there should be two such pairs for the squared "
                         "exponential kernel and five such pairs for the Gibbs "
                         "kernel with tanh length scale warping. If not specified, "
                         "somewhat intelligent guesses are made based on the data "
                         "itself. If you are getting bad fits, try tweaking these. "
                         "Note that this overrides --upper-factor and --lower-factor "
                         "if present.")
parser.add_argument('-i', '--input-filename',
                    help="Filename/path to a CSV or NetCDF file containing the "
                         "profile data to be fit. Note that if you wish to make "
                         "use of the core/edge value, slope constraints you must "
                         "provide t-min and t-max bracketing the times used so "
                         "that the program can find the locations of the magnetic "
                         "axis and GH limiter in the relevant coordinates. "
                         "(Though it will always be able to find the magnetic "
                         "axis if you use a normalized coordinate.) If the "
                         "extension of the file is .csv it will be treated as a "
                         "comma-separated values file, all other extensions will "
                         "be treated as NetCDF files. If using a CSV file, the "
                         "first row should be a comma-separated list of the field "
                         "names, as defined with --abscissa-name and "
                         "--ordinate-name. These columns can be in any order in "
                         "the actual file.")
parser.add_argument('-o', '--output-filename',
                    help="Filename/path to write a NetCDF or CSV file to"
                         "containing the results of the fit. If not specified, "
                         "you will be prompted for a filename upon completing "
                         "the fit.")
parser.add_argument('-x', '--abscissa-name',
                    default='psinorm',
                    help="Name of the variable in the input/output NetCDF/CSV"
                         "files that contains the values of the abscissa "
                         "(independent variable). The uncertainty in the "
                         "abscissa must then be in err_ABSCISSA_NAME, if "
                         "present. Note that uncertainties in the abscissa are "
                         "NOT used in the profile fit at present, but will be "
                         "shown on the plot.")
parser.add_argument('-y', '--ordinate-name',
                    default='y',
                    help="Name of the variable in the input/output NetCDF/CSV "
                         "files that contains the values of the ordinate "
                         "(dependent variable). The uncertainty in the ordinate "
                         "must then be in err_ORDINATE_NAME.")
parser.add_argument('--full-auto',
                    action='store_true',
                    help="Set this flag to disable all prompting for missing/"
                         "optional arguments and run fully automatically. The "
                         "program will exit with status 1 if any required "
                         "parameters are missing.")

args = parser.parse_args()

# Process args, prompt for missing ones:
# Neither shot nor filename specified:
if (not args.input_filename) and (not args.shot):
    if not args.full_auto:
        res = raw_input(
            "No shot number or input file specified. Please type either a shot "
            "number or an input file to proceed.\n\nshotnum/filename = "
        )
        try:
            args.shot = int(res)
        except ValueError:
            args.input_filename = res
    else:
        print("Must specify either a shot number or an input filename.")
        sys.exit(1)

# Turn off edge constraint for --core-only:
if args.core_only:
    args.no_edge_constraint = True
    args.kernel = 'SE'

# Bump random starts up to 4 for low processor count machines:
if not args.random_starts and multiprocessing.cpu_count() < 4:
    print("Number of processors available is less than 4. Setting random_starts "
          "to 4 to help ensure convergence of MAP estimation. You can override "
          "this by setting the --random-starts flag. Running on a computer with "
          "more cores will make this run much faster.")
    args.random_starts = 4

# Handle any missing arguments for when reading from file:
if args.input_filename:
    # Warn user that their input for signal is being ignored:
    if args.signal:
        print("Warning: --input-filename overrides --signal.")
    
    # Warn user that their input for system is being ignored:
    if args.system:
        print("Warning: --input-filename overrides --system.")
    
    # Warn user that their input for robust is being ignored:
    if args.robust:
        print("Warning: --robust has no effect when --input-filename is used.")
    
    # Warn user that their input for --all-points is being ignored:
    if args.all_points:
        print("Warning: --all-points/--no-average has no effect when "
              "--input-filename is used.")
    
    # Get coordinate. Only bug user if the edge or core constraints are in use.
    # TODO: Make this intelligent about reading coordinate from field names!
    if ((not args.coordinate) and
        ((not args.no_edge_constraint) or (not args.no_core_constraint))):
        coordinate = ''
        if not args.full_auto:
            coordinate = raw_input(
                "What coordinate is your data represented as a function of?\n\n"
                "This is needed to apply constraints on the core and the edge. "
                "Leave the line blank and press return to disable these "
                "constraints.\n\n"
                "Valid options are: {psinorm,Rmid,r/a,volnorm,phinorm,"
                "sqrtpsinorm,sqrtr/a,sqrtvolnorm,sqrtphinorm}\n\n"
                "coordinate = "
            )
        if coordinate:
            args.coordinate = coordinate
        else:
            print("No coordinate specified. Disabling core and edge constraints.")
            args.no_edge_constraint = True
            args.no_core_constraint = True
    
    # Get shot number to enable constraints to be fetched. Only bother user if
    # the edge constraint is enabled or the core constraint is enabled and the
    # coordinate is not normalized.    
    if ((not args.shot) and
        ((not args.no_edge_constraint) or
         ((not args.no_core_constraint) and (('norm' not in args.coordinate) and
            ('r/a' not in args.coordinate))))):
        shot = ''
        if not args.full_auto:
            shot = raw_input(
                "What shot number is input file %s from?\n\n"
                "This is needed to apply constraints on the core and the edge. "
                "Leave the line blank and press return to disable these "
                "constraints.\n\n"
                "shot = " % (args.input_filename)
            )
        if shot:
            args.shot = int(shot)
        else:
            print("No shot specified. Disabling edge constraint.")
            args.no_edge_constraint = True
            if ('norm' not in args.coordinate) and ('r/a' not in args.coordinate):
                print("No shot specified and coordinate is not normalized. "
                      "Disabling core constraint.")
                args.no_core_constraint = True
    
    # Get t_min and t_max or t_points. Only bother user if the edge constraint
    # is enabled or the core constraint is enabled and the coordinate is not
    # normalized.
    if (not args.t_min and not args.t_max and not args.t_points and
        ((not args.no_edge_constraint) or
         ((not args.no_core_constraint) and (('norm' not in args.coordinate) and
            ('r/a' not in args.coordinate))))):
        res = ''
        if not args.full_auto:
            res = raw_input(
                "What time window or single time slice does your profile "
                "correspond to?\n\n"
                "This is needed to apply constraints on the core and edge.\n\n"
                "Leave the line blank and press return to disable these "
                "constraints. Enter a single time (in seconds) to use the slice "
                "at that time. Enter two times (in seconds) separated by a space "
                "to average over the window defined by the two times.\n\n"
                "time(s) = "
            )
        if res:
            res_split = res.split()
            if len(res_split) == 1:
                args.t_points = [float(res_split[0])]
            else:
                args.t_min = float(res_split[0])
                args.t_max = float(res_split[1])
        else:
            print("No time(s) specified. Disabling edge constraint.")
            args.no_edge_constraint = True
            if ('norm' not in args.coordinate) and ('r/a' not in args.coordinate):
                print("No time(s) specified and coordinate is not normalized. "
                      "Disabling core constraint.")
                args.no_core_constraint = True
else:
    # Handle case where a shot number is given.
    # Prompt user to specify a signal:
    if not args.signal:
        signal = ''
        if not args.full_auto:
            signal = raw_input(
                "Which signal would you like to fit?\n\n"
                "Valid choices are {ne,Te}.\n\n"
                "signal = "
            )
        if signal in ['Te', 'ne']:
            args.signal = signal
        else:
            print("Signal to fit not specified or invalid!")
            sys.exit(1)
    
    # Warn user they haven't specified a time range/point:
    if ((not args.t_min) or (not args.t_max)) and (not args.t_points):
        res = ''
        if not args.full_auto:
            res = raw_input(
                "What time window or single time slice do you want to use?\n\n"
                "Leave the line blank and press return to use all points in the "
                "shot (not recommended). Enter a single time (in seconds) to use "
                "the slice at that time. Enter two times (in seconds) separated "
                "by a space to average over the window defined by the two times.\n\n"
                "time(s) = "
            )
            if res:
                res_split = res.split()
                if len(res_split) == 1:
                    args.t_points = [float(res_split[0])]
                else:
                    args.t_min = float(res_split[0])
                    args.t_max = float(res_split[1])
        if not res:
            print("Warning: Not specifying t-min and t-max or t-points causes the "
                  "entire time-history to be used. This is usually not desirable.")
    
    # Handle default value for coordinate:
    if not args.coordinate:
        args.coordinate = 'psinorm'

# Fetch data:
if args.input_filename:
    # TODO: Build this functionality into profiletools!
    # Read input file:
    root, ext = os.path.splitext(args.input_filename)
    if ext.lower() == '.csv':
        print("Fetching data from CSV file %s..." % (args.input_filename,))
        # TODO: Re-write this to call read_plasma_csv!
        # TODO: Add appropriate flags to specify bivariate data with read_plasma_csv!
        X = []
        y = []
        err_X = []
        err_y = []
        with open(os.path.expanduser(args.input_filename), 'r') as infile:
            rdr = csv.DictReader(infile)
            for row in rdr:
                X.append(float(row[args.abscissa_name]))
                try:
                    err_X.append(float(row['err_'+args.abscissa_name]))
                except KeyError:
                    err_X.append(0)
                y.append(float(row[args.ordinate_name]))
                try:
                    err_y.append(float(row['err_'+args.ordinate_name]))
                except KeyError:
                    err_y.append(0)
        # TODO: Parse this out of the headers!
        X_units = ''
        y_units = ''
    else:
        print("Fetching data from NetCDF file %s..." % (args.input_filename,))
        with scipy.io.netcdf.netcdf_file(os.path.expanduser(args.input_filename), mode='r') as infile:
            vX = f.variables[args.abscissa_name]
            X = vX[:]
            try:
                X_units = vX.units
            except AttributeError:
                X_units = ''
            try:
                err_X = f.variables['err_'+args.abscissa_name]
            except KeyError:
                err_X = 0
        
            vy = f.variables[args.ordinate_name]
            y = vy[:]
            try:
                y_units = vy.units
            except AttributeError:
                y_units = ''
            try:
                err_y = f.variables['err_'+args.ordinate_name]
            except KeyError:
                err_y = 0
    
    p = profiletools.BivariatePlasmaProfile(
        X_dim=1, X_units=X_units, y_units=y_units,
        X_labels=args.abscissa_name, y_label=args.ordinate_name
    )
    p.add_data(X, y, err_X=err_X, err_y=err_y)
    p.shot = args.shot
    p.abscissa = args.coordinate
    if args.t_points:
        p.times = args.t_points
        p.t_min = min(args.t_points)
        p.t_max = max(args.t_points)
    else:
        p.t_min = args.t_min
        p.t_max = args.t_max
    if args.shot:
        p.efit_tree = eqtools.CModEFITTree(args.shot)
else:
    # Create relevant object:
    profile_kwargs = {}
    if args.system:
        systems = set(args.system)
        if 'TS' in systems:
            systems.remove('TS')
            systems.add('ETS')
            systems.add('CTS')
        profile_kwargs['include'] = systems
    print("Fetching data from tree...")
    if args.signal == 'ne':
        p = profiletools.ne(args.shot, abscissa=args.coordinate,
                            t_min=args.t_min, t_max=args.t_max, **profile_kwargs)
    elif args.signal == 'Te':
        p = profiletools.Te(args.shot, abscissa=args.coordinate,
                            t_min=args.t_min, t_max=args.t_max, **profile_kwargs)

# Take care of averaging -- do it here to make it easier to support bivariate
# profiles from files in the future:
if p.X_dim > 1:
    if args.t_points:
        p.keep_times(args.t_points)
        if len(args.t_points) == 1 or args.all_points:
            p.drop_axis(0)
        else:
            p.time_average(robust=args.robust)
    else:
        if args.all_points:
            p.drop_axis(0)
        else:
            p.time_average(robust=args.robust)

if args.core_only:
    print("Removing edge points...")
    p.remove_edge_points()

if args.change_threshold:
    print("Removing points that exhibit extreme changes...")
    X_bad_c, y_bad_c, err_X_bad_c, err_y_bad_c = \
        p.remove_extreme_changes(thresh=args.change_threshold)
    X_bad_c = scipy.asarray(X_bad_c).flatten()
    err_X_bad_c = scipy.asarray(err_X_bad_c).flatten()
    print("Removed %d points with extreme changes." % (len(y_bad_c),))

print("Creating Gaussian process...")
p.create_gp(k=args.kernel,
            constrain_slope_on_axis=not args.no_core_constraint,
            constrain_at_limiter=not args.no_edge_constraint,
            upper_factor=args.upper_factor, lower_factor=args.lower_factor)
if args.bounds:
    new_bounds = scipy.asarray(args.bounds)
    if len(new_bounds) != len(p.gp.k.hyperprior.bounds.flatten()):
        print("Incorrect number of lower, upper bound pairs!")
        sys.exit(1)
    p.gp.k.hyperprior.bounds = new.bounds.reshape((-1, 2))
print("Finding MAP estimate of hyperparameters...")
p.find_gp_MAP_estimate(random_starts=args.random_starts, verbose=True)
if args.outlier_threshold:
    print("Removing outliers...")
    X_bad_o, y_bad_o, err_X_bad_o, err_y_bad_o = \
        p.remove_outliers(thresh=args.outlier_threshold)
    X_bad_o = scipy.asarray(X_bad_o).flatten()
    err_X_bad_o = scipy.asarray(err_X_bad_o).flatten()
    print("Removed %d outliers." % (len(y_bad_o),))
    print("Finding MAP estimate of hyperparameters with outliers removed...")
    p.find_gp_MAP_estimate(random_starts=args.random_starts, verbose=True)

f = plt.figure()
gs = mplgs.GridSpec(3, 2, width_ratios=[8, 1])
a_val = f.add_subplot(gs[:, 0])
# a_grad = f.add_subplot(gs[1, 0], sharex=a_val)
# a_a_L = f.add_subplot(gs[2, 0], sharex=a_val)
# a_adjust = f.add_subplot(gs[0, 1])
a_reject = f.add_subplot(gs[1, 1])
a_accept = f.add_subplot(gs[2, 1])

print("Plotting data...")
p.plot_data(ax=a_val)

if args.change_threshold:
    a_val.errorbar(X_bad_c, y_bad_c, yerr=err_y_bad_c, xerr=err_X_bad_c,
                   fmt='mx', label='extreme change')
if args.outlier_threshold:
    a_val.errorbar(X_bad_o, y_bad_o, yerr=err_y_bad_o, xerr=err_X_bad_o,
                   fmt='rx', label='outlier')

print("Evaluating at desired points and plotting result...")
if args.x_pts:
    X = args.x_pts
else:
    X = scipy.linspace(args.x_min, args.x_max, args.npts)
ax, mean, std = p.plot_gp(ax=a_val, X=X, label='MAP', return_prediction=True)

a_val.legend(loc='best')

# Create reject button:
def reject_and_quit(evt):
    print("Rejecting fit and exiting without writing file.")
    sys.exit(2)
b_reject = mplw.Button(a_reject, 'reject and exit')
b_reject.on_clicked(reject_and_quit)

# Create accept button:
def accept_and_quit(evt):
    if not args.output_filename:
        args.output_filename = raw_input(
            "Please type the file name/path for the file you want to save the "
            "fit to. If the file exists, it will be overwritten without warning.\n\n"
            "filename = "
        )
    root, ext = os.path.splitext(args.output_filename)
    history = (
        "Created by user {user} on {host} with {module} version {ver} on {time}. "
        "Command used was:\n{argv}\n".format(
            host=socket.gethostname(), user=getpass.getuser(),
            module=inspect.stack()[0][1], ver=__version__, time=time.asctime(),
            argv=sys.argv
        )
    )
    if args.input_filename:
        X_name = args.abscissa_name
    else:
        X_name = args.coordinate
    if ext.lower() == '.csv':
        with open(os.path.expanduser(args.output_filename), 'wb') as outfile:
            outfile.write(history)
            writer = csv.writer(outfile)
            writer.writerow([X_name, args.ordinate_name, 'err_' + args.ordinate_name])
            writer.writerows(zip(X, mean, std))
    else:
        with scipy.io.netcdf.netcdf_file(os.path.expanduser(args.output_filename), mode='w') as f:
            f.history = history
            f.createDimension(X_name, len(X))
            v_X = f.createVariable(X_name, float, (X_name,))
            v_X[:] = X
            v_y = f.createVariable(args.ordinate_name, float, (X_name,))
            v_y[:] = mean
            v_err_y = f.createVariable('err_'+args.ordinate_name, float, (X_name,))
            v_err_y[:] = std
    sys.exit(0)
b_accept = mplw.Button(a_accept, 'accept, save and exit')
b_accept.on_clicked(accept_and_quit)

print("Done.")

plt.show()
